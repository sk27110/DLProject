{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a17c98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dataset import get_datasets\n",
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "from src.eval import evaluate_model\n",
    "from src.models import ImageCaptioningModelTransformer  # твоя модель\n",
    "import torch.nn as nn\n",
    "from src.dataset import get_datasets\n",
    "import timm\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8896de48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(checkpoint_path, train, device=\"cuda\"):\n",
    "\n",
    "    vit_model = timm.create_model('vit_small_patch16_224', pretrained=True)\n",
    "    vit_model.head = torch.nn.Identity()\n",
    "    for p in vit_model.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    vit_model.eval()\n",
    "\n",
    "    model = ImageCaptioningModelTransformer(train.tokenizer.vocab_size, vit_model)\n",
    "\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "\n",
    "    state_dict = checkpoint[\"model_state\"] if \"model_state\" in checkpoint else checkpoint[\"model_state_dict\"]\n",
    "    model.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96239de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "checkpoint = \"./checkpoint/run1_checkpoint_epoch_19.pth\"\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "train, _, test = get_datasets()\n",
    "test_loader = DataLoader(test, batch_size=8, shuffle=False)\n",
    "\n",
    "model = load_model(checkpoint, train, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ecfe077",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "image = Image.open(\"./test_img/girl.jpg\").convert(\"RGB\")\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "image = transform(image).unsqueeze(0).to(device)  # [1, 3, 224, 224]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ada16a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(sz):\n",
    "    \"\"\"\n",
    "    Создаёт маску для Transformer decoder,\n",
    "    чтобы предотвратить \"заглядывание\" на будущие токены\n",
    "    \"\"\"\n",
    "    mask = torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c212b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated caption: felix sneakersronezzle sliced colliery ter patterson 山 ය fr clarify glenn ter patterson 山 ය fr clarify glenn\n"
     ]
    }
   ],
   "source": [
    "max_len = 20  # максимальная длина генерируемой подписи\n",
    "generated = [tokenizer.cls_token_id]  # стартовый токен\n",
    "\n",
    "for _ in range(max_len):\n",
    "    # создаём маску для текущей последовательности\n",
    "    tgt_mask = generate_square_subsequent_mask(len(generated)).to(device)\n",
    "\n",
    "    # превращаем текущие токены в тензор\n",
    "    inp = torch.tensor(generated).unsqueeze(0).to(device)  # [1, seq_len]\n",
    "\n",
    "    # forward pass через модель\n",
    "    output = model(image, inp, tgt_mask)  # [1, seq_len, vocab_size]\n",
    "\n",
    "    # берём последний токен (самый вероятный)\n",
    "    next_token = output.argmax(-1)[0, -1].item()\n",
    "\n",
    "    # добавляем его в последовательность\n",
    "    generated.append(next_token)\n",
    "\n",
    "    # если встретили конец предложения, останавливаем генерацию\n",
    "    if next_token == tokenizer.sep_token_id:\n",
    "        break\n",
    "\n",
    "# преобразуем токены обратно в текст\n",
    "caption = tokenizer.decode(generated, skip_special_tokens=True)\n",
    "print(\"Generated caption:\", caption)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ccc45f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30522, 256])\n",
      "30522\n"
     ]
    }
   ],
   "source": [
    "print(model.word_embedding.weight.shape)\n",
    "print(tokenizer.vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80d4b200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sasha/.cache/kagglehub/datasets/adityajn105/flickr8k/versions/1/Images/1000268201_693b08cb0e.jpg\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.4851,  0.4508,  0.3138,  ..., -1.8953, -1.8610, -1.8097],\n",
       "          [ 0.4851,  0.4508,  0.3309,  ..., -1.8439, -1.9295, -1.5699],\n",
       "          [ 0.5022,  0.4508,  0.3652,  ..., -1.7412, -1.8610, -1.9295],\n",
       "          ...,\n",
       "          [ 1.3927,  1.0502,  1.3070,  ..., -1.0390, -1.1418,  0.0056],\n",
       "          [ 1.6495,  1.3070,  1.1529,  ..., -1.0904, -1.1589,  0.2967],\n",
       "          [ 1.4440,  1.3755,  1.0331,  ..., -1.0904, -1.1760,  0.5022]],\n",
       " \n",
       "         [[ 0.8354,  0.8354,  0.6779,  ..., -1.5805, -1.6506, -1.5630],\n",
       "          [ 0.8179,  0.8354,  0.6779,  ..., -1.7381, -1.7206, -1.2129],\n",
       "          [ 0.8529,  0.8354,  0.6779,  ..., -1.7206, -1.8081, -1.6155],\n",
       "          ...,\n",
       "          [ 0.5028,  0.2752,  0.5378,  ..., -0.0749, -0.2150,  0.7654],\n",
       "          [ 0.8880,  0.8880,  0.4503,  ..., -0.1099, -0.2850,  0.9055],\n",
       "          [ 0.3102,  0.2752, -0.1099,  ..., -0.1099, -0.3025,  1.1506]],\n",
       " \n",
       "         [[ 0.6879,  0.6705,  0.4962,  ..., -1.6476, -1.6650, -1.5953],\n",
       "          [ 0.5834,  0.5834,  0.5311,  ..., -1.6999, -1.6824, -1.4907],\n",
       "          [ 0.6008,  0.6008,  0.4962,  ..., -1.6824, -1.6824, -1.6999],\n",
       "          ...,\n",
       "          [-0.6193, -0.6018, -0.5495,  ...,  0.9668,  0.8622,  1.0365],\n",
       "          [-0.2184, -0.2184, -0.4101,  ...,  0.8971,  0.7925,  1.1062],\n",
       "          [-0.4450, -0.6715, -0.9853,  ...,  0.8797,  0.7751,  1.2631]]]),\n",
       " tensor([  101,  1037,  2210,  2611,  8218,  2046,  1037,  4799, 17408,  1012,\n",
       "           102,     0,     0,     0,     0,     0,     0,     0,     0,     0]),\n",
       " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125b07aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.train import Trainer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
